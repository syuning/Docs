## Setting up a data lake
## 配置一个数据湖

> This feature is technical preview.  
> 此功能是技术预览。
 
A **data lake** provides a way for you to centrally apply and enforce authentication, authorization, and audit policies across multiple ephemeral workload clusters. "Attaching" your workload cluster to the data lake instance allows the attached cluster workloads to access data and run in the security context provided by the data lake. 
**数据湖** 为您提供了一种跨多个临时工作负载集集群中应用和实施身份验证，授权和审核策略的方法。 将工作负载集群“附加”到数据湖实例允许附加的集群工作负载访问数据并在数据湖提供的安全上下文中运行。

While workloads are temporary, the security policies around your data schema are long-running and shared for all workloads. As your workloads come and go, the data lake instance lives on, providing consistent and available security policy definitions that are available for current and future ephemeral workloads. All information related to schema (Hive), security policies (Ranger) and audit (Ranger) is stored on external locations (external databases and cloud storage).
虽然工作负载是临时的，但围绕数据模式的安全策略是长期运行的，并且对所有工作负载都是共享的。 当您的工作负载来来去去时，数据湖实例依然存在，提供可用于当前和未来短暂工作负载的一致且可用的安全策略定义。 与架构（Hive），安全策略（Ranger）和审计（Ranger）相关的所有信息都存储在外部位置（外部数据库和云存储）中。

This is illustrated in the following diagram: 
如下图所示：

<a href="../images/cb_datalake-diag01.png" target="_blank" title="click to enlarge"><img src="../images/cb_datalake-diag01.png" width="550" title="Cloudbreak architecture"></a>

[Source]: <> (Source https://docs.google.com/presentation/d/1vo3aZVMX0vx9gHZ5hALgxkGHw3l1PW-bGbxaYjk57Uo/edit#slide=id.p1)

Once you’ve created a data lake instance, you have an option to attach it to one or more ephemeral clusters. This allows you to apply the authentication, authorization, and audit across multiple workload clusters.
创建数据湖实例后，您可以选择将其附加到一个或多个临时集群。 这允许您跨多个工作负载集群应用身份验证，授权和审计。

The following table explains basic data lake terminology: 
下表解释了基本数据湖术语：

| 项 Term | 描述 Description |
|---|---|
| 数据湖 Data lake | 运行Ranger，用于配置授权。 策略并用于审计捕获。 运行Hive Metastore，用于数据模式。 Runs Ranger, which is used for configuring authorization. policies and is used for audit capture. Runs Hive Metastore, which is used for data schema. |
| 工作负载集群 Workload clusters | 连接到数据湖以运行工作负载的集群。 这是您通过[JDBC]（hive.md）运行Hive等工作负载的地方。 The clusters that get attached to the data lake to run workloads. This is where you run workloads such as Hive via [JDBC](hive.md). |

The following table explains the components of a data lake: 
下表说明了数据湖的组件：

<a href="../images/cb_datalake-diag02.png" target="_blank" title="click to enlarge"><img src="../images/cb_datalake-diag02.png" width="550" title="Cloudbreak architecture"></a>

[Source]: <> (Source https://docs.google.com/presentation/d/1vo3aZVMX0vx9gHZ5hALgxkGHw3l1PW-bGbxaYjk57Uo/edit#slide=id.p1)

| Component | Technology | Description |
|---|---|---|
| Schema | Apache Hive | 提供Hive架构（表，视图等）。 如果您有两个或更多工作负载访问相同的Hive数据，则需要跨这些工作负载共享模式。Provides Hive schema (tables, views, and so on). If you have two or more workloads accessing the same Hive data, you need to share schema across these workloads. |
| Policy | Apache Ranger | 定义Hive架构周围的安全策略。 如果您有两个或更多用户访问相同的数据，则需要始终可用并强制执行安全策略。Defines security policies around Hive schema. If you have two or more users accessing the same data, you need security policies to be consistently available and enforced. |
| Audit | Apache Ranger | 审核用户访问权限并捕获工作负载的数据访问活动。Audits user access and captures data access activity for the workloads. |
| [Directory ](external-ldap.md)| LDAP/AD | 为用户提供身份验证源以及为授权提供组的定义。Provides an authentication source for users and a definition of groups for authorization. |
| [Gateway](gateway.md) | Apache Knox | 支持可以使用SSL保护并启用身份验证以访问资源的单个工作负载端点。Supports a single workload endpoint that can be protected with SSL and enabled for authentication to access to resources. |

### Overview of data lake setup 
### 数据湖设置概览

Setting up a data lake involves the following steps:
设置数据湖包括以下步骤：

| Step | Where to perform |
|---|---|
| [满足先决条件Meet the prerequisites](#prerequisites):<ul><li>创建两个外部数据库（一个用于Hive Metastore，一个用于Ranger）Create two external databases (one for Hive metastore and one for Ranger)</li><li>为LDAP / AD创建外部身份验证源Create an external authentication source for LDAP/AD</li><li>准备云存储位置（取决于您的云供应商，应该是在Amazon S3上，Azure的ADLS或WASB或GCS），以获取默认的Hive仓库目录和Ranger审计日志）Prepare a cloud storage location (depending on your cloud provider, this should be, on Amazon S3, Azure's ADLS or WASB, or GCS) for default Hive warehouse directory and Ranger audit logs)</li></ul>|您必须在Cloudbreak之外自行创建这些资源。 您可以使用一个数据库实例并创建两个数据库。You must create these resources on your own, outside of Cloudbreak. You may use one database instance and create two databases.|
| [注册两个数据库和LDAPRegister the two databases and LDAP](#register-the-databases-and-ldap-in-the-cloudbreak-web-ui) | 在Cloudbreak Web UI>外部源中In the Cloudbreak web UI > External Sources |
| [创建一个数据湖Create a data lake](#create-a-data-lake) | 在Cloudbreak Web UI>创建集群中In the Cloudbreak web UI > Create cluster |
| [创建附加到数据湖的集群Create clusters attached to the data lake](#create-attached-hdp-clusters) | 在Cloudbreak Web UI>创建集群中In the Cloudbreak web UI > Create cluster |

### Prerequisites
### 前提

You must perform the following prerequisites *outside of Cloudbreak*: 
您必须在Cloudbreak *之外执行以下先决条件*：

1. Set up two external database instances, one for the HIVE component, and one for the RANGER component. For supported databases, refer to [Using an external database](external-db.md#supported-databases).    
1.设置两个外部数据库实例，一个用于HIVE组件，另一个用于RANGER组件。 有关受支持的数据库，请参阅[使用外部数据库]（external-db.md＃supported-databases）。
2. Create an LDAP instance and set up your users inside the LDAP.  
2.创建LDAP实例并在LDAP中设置用户。
3. Prepare a cloud storage location for default Hive warehouse directory and Ranger audit logs.  
3.为默认Hive仓库目录和Ranger审核日志准备云存储位置。

In the steps that follow, you will be required to provide the information related to these external resources. 
在接下来的步骤中，您将需要提供与这些外部资源相关的信息。

### Register the databases and LDAP in the Cloudbreak web UI
### 在Cloudbreak Web UI中注册数据库和LDAP

Prior to creating a data lake, you must register the following resources in the Cloudbreak web UI:
在创建数据湖之前，您必须在Cloudbreak Web UI中注册以下资源：

**步骤** 

1. Register each of your two RDS instances created as part of the prerequisites in the Cloudbreak web UI, under **External Sources** > **Database Configurations**. For instructions, refer  [Using an external database](external-db.md#register-an-external-database).    
1.在“外部源**”**数据库配置**下，注册作为Cloudbreak Web UI中先决条件的一部分创建的两个RDS实例。 有关说明，请参阅[使用外部数据库]（external-db.md #register-an-external-database）。
    * When registering the database for Hive, select **Type > Hive**.  
    * 在为Hive注册数据库时，选择** Type> Hive **。
    * When registering the database for Ranger, select **Type > Ranger**.  
    * 为Ranger注册数据库时，选择** Type> Ranger **。

2. Register your LDAP (created as part of the prerequisites) in the Cloudbreak web UI, under **External Sources > Authentication Configurations**. For instructions, refer to [Register an authentication source](external-ldap.md#register-an-authentication-source). 
2. 在Cloudbreak Web UI中的**外部源>身份验证配置**下注册LDAP（作为先决条件的一部分创建）。 有关说明，请参阅[注册认证源]（external-ldap.md #register-an-authentication-source）。

As an outcome of this step, you should have two external databases and one authentication source registered in the Cloudbreak web UI.  
作为此步骤的结果，您应该在Cloudbreak Web UI中注册两个外部数据库和一个身份验证源。


### Create a data lake 
### 创建一个数据湖

Create a data lake by using the create cluster wizard. Among other information, make sure to provide the information listed in the steps below. 
使用create cluster wizard创建数据湖。 除其他信息外，请务必提供以下步骤中列出的信息。

**步骤** 

1. Navigate to the advanced version of the create cluster wizard.  
1. 导航到创建集群向导的高级版本。
2. On the **General Configuration** page:  
2. 在**常规配置**页面上：
    * Under **Cluster Name**, provide a name for your data lake. 
    * 在**群组名称**下，为您的数据湖提供名称。
    * Under **Cluster Type**, choose the **DATA LAKE** blueprint:
    * 在** Cluster Type **下，选择** DATA LAKE **蓝图：

    <a href="../images/cb_datalake01.png" target="_blank" title="click to enlarge"><img src="../images/cb_datalake01.png" width="650" title="Cloudbreak web UI"></a>  
    
3. On the **Cloud Storage** page:  
3. 在**云存储**页面上：
    * Under **Cloud Storage**, configure access to cloud storage via the method available for your cloud provider.  
    * 在**云存储**下，通过云供应商可用的方法配置对云存储的访问。
    * Under **Storage Locations**, provide an existing location within your cloud storage account that can be used for Hive warehouse directory and Ranger audit logs.  
    * 在**存储位置**下，提供云存储帐户中可用于Hive仓库目录和Ranger审核日志的现有位置。

    <div class="danger">
    <p class="first admonition-title">Note</p>
    <p class="last">The storage location must exist prior to data lake provisioning. If the storage location does not exist then Ranger is installed properly, but it may not work.</p>
</div>

    <a href="../images/cb_datalake02.png" target="_blank" title="click to enlarge"><img src="../images/cb_datalake02.png" width="650" title="Cloudbreak web UI"></a>  
   
4. On the **External Sources** page, select the previously registered Ranger database, Hive database and LDAP:
4. 在** External Sources **页面上，选择以前注册的Ranger数据库，Hive数据库和LDAP：

    <a href="../images/cb_datalake03.png" target="_blank" title="click to enlarge"><img src="../images/cb_datalake03.png" width="650" title="Cloudbreak web UI"></a> 
    
5. On the **Gateway Configuration** page, the gateway is enabled by default with Ambari exposed through the gateway. You should also enable Ranger by selecting the Ranger service and clicking **Expose**. 
5. 在** Gateway Configuration **页面上，默认情况下启用网关，Ambari通过网关公开。 您还应该通过选择Ranger服务并单击** Expose **来启用Ranger。

    <a href="../images/cb_datalake04.png" target="_blank" title="click to enlarge"><img src="../images/cb_datalake04.png" width="650" title="Cloudbreak web UI"></a>    
    
6. On the **Security** page:
6. 在**安全**页面上：
    * Under **Password**, provide a strong password for your cluster. For example “SomeRandomChars123!” is a strong password. A strong password is required for the default Ranger admin, which - among other cluster components like Ambari - will use this password. 
    * 在**密码**下，为您的集群提供强密码。 例如，“SomeRandomChars123！”是一个强密码。 默认Ranger管理员需要一个强密码，其中包括Ambari等其他集群组件将使用此密码。
    * Select an SSH key.
    * 选择SSH密钥。

7. Click **CREATE CLUSTER** to initiate data lake creation.       
7. 单击** CREATE CLUSTER **以启动数据湖创建。
 
As an outcome of this step, you should have a running data lake.
作为此步骤的结果，您应该有一个运行数据湖。


### Create attached HDP clusters
### 创建附加的HDP集群

Once your data lake is running, you can start creating clusters attached to the data lake. Follow these general steps to create an cluster attached to a data lake. 
数据湖运行后，您可以开始创建附加到数据湖的集群。 按照以下常规步骤创建附加到数据湖的集群。

In general, once you've selected the data lake that the cluster should be using, the cluster wizard should provide you with the cluster settings that should be used for the attached cluster.    
通常，一旦选择了集群应使用的数据湖，集群向导就应该为您提供应该用于连接集群的集群设置。

**步骤** 

1. In the Cloudbreak web UI, click on the cluster tile representing your data lake.  
1. 在Cloudbreak Web UI中，单击表示数据湖的集群磁贴。

2. From the **ACTIONS** menu, select **CREATE ATTACHED CLUSTER**.    
2. 从** ACTIONS **菜单中，选择** CREATE ATTACHED CLUSTER **。
 
    <a href="../images/cb_datalake05.png" target="_blank" title="click to enlarge"><img src="../images/cb_datalake05.png" width="350" title="Cloudbreak web UI"></a> 

3. In general, the cluster wizard should provide you with the cluster settings that should be used for the attached cluster. Still, make sure to do the following:
3. 通常，集群向导应为您提供应用于连接集群的集群设置。 不过，请务必执行以下操作：

    * Under **Region** and **Availability Zone**, select the same location where your data lake is running.   
    * 在 ** Region ** 和** Availability Zone ** 下，选择运行数据湖的位置。
    * Select one of the three default blueprints.  
    * 选择三个默认蓝图中的一个。
    * On the **Cloud Storage** page, enter the same cloud storage location that your data lake is using.  
    * 在** Cloud Storage **页面上，输入您的数据湖正在使用的云存储位置。
    * On the **External Sources** page, the LDAP, and Ranger and Hive databases that you attached to the data lake should be attached to your cluster.      
    * 在** External Sources **页面上，您附加到数据湖的LDAP，Ranger和Hive数据库应附加到您的集群。
    * On the **Network** page, select the same VPC and subnet where the data lake is running.  
    * 在** Network **页面上，选择运行数据湖的相同VPC和子网。

4. Click on **CREATE CLUSTER** to initiate cluster creation. 
4. 单击** CREATE CLUSTER **以启动集群创建。

As an outcome of this step, you should have a running cluster attached to the data lake. Access your attached clusters and run your workloads as normal.
作为此步骤的结果，您应该有一个连接到数据湖的正在运行的集群。 访问连接的集群并正常运行工作负载。


